{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ac1bb9",
   "metadata": {},
   "source": [
    "# Notebook 1: Exploratory Data Analysis (EDA) and Data Preparation\n",
    "## MIMIC-III Clinical Database for Recommendation Systems\n",
    "\n",
    "**Author:** Data Science Team  \n",
    "**Date:** November 2025  \n",
    "**Objective:** Complete EDA with data loading, inspection, cleaning, and train-test split\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ          EDA WORKFLOW PIPELINE                       ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ 1. DATA LOADING                                      ‚îÇ\n",
    "‚îÇ    ‚Üì                                                 ‚îÇ\n",
    "‚îÇ 2. INITIAL INSPECTION (head, info, describe)        ‚îÇ\n",
    "‚îÇ    ‚Üì                                                 ‚îÇ\n",
    "‚îÇ 3. MISSING VALUES & DUPLICATES                       ‚îÇ\n",
    "‚îÇ    ‚Üì                                                 ‚îÇ\n",
    "‚îÇ 4. OUTLIERS DETECTION                                ‚îÇ\n",
    "‚îÇ    ‚Üì                                                 ‚îÇ\n",
    "‚îÇ 5. EXPLORATORY ANALYSIS (Distributions)             ‚îÇ\n",
    "‚îÇ    ‚Üì                                                 ‚îÇ\n",
    "‚îÇ 6. CORRELATION ANALYSIS                              ‚îÇ\n",
    "‚îÇ    ‚Üì                                                 ‚îÇ\n",
    "‚îÇ 7. CLASS BALANCING ASSESSMENT                        ‚îÇ\n",
    "‚îÇ    ‚Üì                                                 ‚îÇ\n",
    "‚îÇ 8. DATA CLEANING & PREPROCESSING                     ‚îÇ\n",
    "‚îÇ    ‚Üì                                                 ‚îÇ\n",
    "‚îÇ 9. TRAIN-TEST SPLIT & EXPORT                         ‚îÇ\n",
    "‚îÇ    ‚Üì                                                 ‚îÇ\n",
    "‚îÇ 10. FINAL SUMMARY REPORT                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340e1d9",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7787254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for EDA and data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Configure visualization\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print('‚úì All libraries imported successfully!')\n",
    "print(f'Execution timestamp: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8cd0e",
   "metadata": {},
   "source": [
    "## Section 2: Load Dataset from MIMIC-III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MIMIC-III clinical database tables\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "DATASET_NAME = 'ihssanened/mimic-iii-clinical-databaseopen-access'\n",
    "\n",
    "def load_mimic_table(file_name, date_col=None):\n",
    "    \"\"\"Load a single MIMIC-III table using KaggleHub.\"\"\"\n",
    "    try:\n",
    "        print(f'  Loading: {file_name}...', end=' ')\n",
    "        df = kagglehub.load_dataset(\n",
    "            KaggleDatasetAdapter.PANDAS,\n",
    "            DATASET_NAME,\n",
    "            file_name\n",
    "        )\n",
    "        if date_col and date_col in df.columns:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        print(f'‚úì {df.shape[0]:,} rows √ó {df.shape[1]} columns')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'‚úó Error loading {file_name}')\n",
    "        return None\n",
    "\n",
    "print('\\n=== Loading MIMIC-III Tables ===\\n')\n",
    "df_admissions = load_mimic_table('admissions.csv', date_col='admittime')\n",
    "df_patients = load_mimic_table('patients.csv')\n",
    "df_labevents = load_mimic_table('labevents.csv')\n",
    "df_d_labitems = load_mimic_table('d_labitems.csv')\n",
    "\n",
    "if df_admissions is None or df_patients is None:\n",
    "    print('\\n‚ùå Critical tables failed to load!')\n",
    "else:\n",
    "    print('\\n‚úì All critical tables loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b26186",
   "metadata": {},
   "source": [
    "## Section 3: Initial Data Inspection (head, info, describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform initial inspection of key tables\n",
    "def inspect_table(df, name):\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'TABLE: {name.upper()}')\n",
    "    print(f'{\"=\"*80}')\n",
    "    print(f'\\nShape: {df.shape[0]:,} rows √ó {df.shape[1]} columns')\n",
    "    print(f'\\nFirst 5 rows:')\n",
    "    display(df.head())\n",
    "    print(f'\\nData types and missing values:')\n",
    "    print(df.info())\n",
    "    print(f'\\nDescriptive statistics:')\n",
    "    display(df.describe(include='all').round(2))\n",
    "\n",
    "if df_admissions is not None:\n",
    "    inspect_table(df_admissions, 'admissions')\n",
    "if df_patients is not None:\n",
    "    inspect_table(df_patients, 'patients')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a9a1b",
   "metadata": {},
   "source": [
    "## Section 4: Missing Values and Duplicates Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values\n",
    "def analyze_missing(df, name):\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'Missing Values Analysis: {name.upper()}')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing_Count': df.isnull().sum(),\n",
    "        'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    }).sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    print(f'\\nTotal missing: {df.isnull().sum().sum():,}')\n",
    "    display(missing_summary.head(10))\n",
    "    \n",
    "    # Visualization\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False).head(10)\n",
    "    if len(missing_pct) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        missing_pct.plot(kind='barh', ax=ax, color='coral')\n",
    "        ax.set_xlabel('Missing Value Percentage (%)', fontweight='bold')\n",
    "        ax.set_title(f'Missing Values Distribution: {name.upper()}', fontsize=12, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f'\\n**Interpretation:** This chart shows the percentage of missing values for each column.')\n",
    "        print(f'High percentages indicate sparse data, which is common in clinical datasets.')\n",
    "\n",
    "# Analyze duplicates\n",
    "def analyze_duplicates(df, name):\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'Duplicates Analysis: {name.upper()}')\n",
    "    print(f'{\"=\"*80}')\n",
    "    dup_count = df.duplicated().sum()\n",
    "    print(f'Total duplicate rows: {dup_count}')\n",
    "    print(f'Duplicate percentage: {(dup_count/len(df)*100):.2f}%')\n",
    "\n",
    "if df_admissions is not None:\n",
    "    analyze_missing(df_admissions, 'admissions')\n",
    "    analyze_duplicates(df_admissions, 'admissions')\n",
    "\n",
    "if df_labevents is not None:\n",
    "    analyze_missing(df_labevents, 'labevents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4d5ae",
   "metadata": {},
   "source": [
    "## Section 5: Outliers Detection using Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d82784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and visualize outliers\n",
    "if df_labevents is not None and 'valuenum' in df_labevents.columns:\n",
    "    print('\\nOutliers Detection in Laboratory Values')\n",
    "    print('='*80)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Box plot for outliers\n",
    "    sns.boxplot(y=df_labevents['valuenum'].dropna(), ax=axes[0], color='lightblue')\n",
    "    axes[0].set_title('Box Plot: Laboratory Values (Outliers Detection)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Laboratory Value')\n",
    "    \n",
    "    # Histogram with outlier regions\n",
    "    Q1 = df_labevents['valuenum'].quantile(0.25)\n",
    "    Q3 = df_labevents['valuenum'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    axes[1].hist(df_labevents['valuenum'].dropna(), bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(lower_bound, color='red', linestyle='--', linewidth=2, label='Outlier Boundary')\n",
    "    axes[1].axvline(upper_bound, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1].set_title('Histogram: Laboratory Values with Outlier Boundaries', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Laboratory Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    outlier_count = ((df_labevents['valuenum'] < lower_bound) | (df_labevents['valuenum'] > upper_bound)).sum()\n",
    "    print(f'\\n**Interpretation:**')\n",
    "    print(f'‚Ä¢ Total values: {len(df_labevents[\"valuenum\"].dropna()):,}')\n",
    "    print(f'‚Ä¢ Outliers detected (IQR method): {outlier_count:,} ({outlier_count/len(df_labevents[\"valuenum\"].dropna())*100:.2f}%)')\n",
    "    print(f'‚Ä¢ Lower bound: {lower_bound:.2f}')\n",
    "    print(f'‚Ä¢ Upper bound: {upper_bound:.2f}')\n",
    "    print(f'‚Ä¢ Outliers are values beyond 1.5√óIQR from Q1 and Q3, indicated by the red dashed lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe390e",
   "metadata": {},
   "source": [
    "## Section 6: Exploratory Analysis - Distributions and Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81589aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distributions\n",
    "print('\\nExploratory Data Analysis: Variable Distributions')\n",
    "print('='*80)\n",
    "\n",
    "if df_labevents is not None and 'valuenum' in df_labevents.columns:\n",
    "    print('\\n[1] Distribution of Laboratory Values')\n",
    "    lab_values = df_labevents['valuenum'].dropna()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0, 0].hist(lab_values, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_title('Histogram: Distribution of Laboratory Values', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Laboratory Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # KDE plot\n",
    "    lab_values.plot(kind='kde', ax=axes[0, 1], color='steelblue', linewidth=2)\n",
    "    axes[0, 1].fill_between(axes[0, 1].get_lines()[0].get_xdata(), \n",
    "                            axes[0, 1].get_lines()[0].get_ydata(), alpha=0.3, color='steelblue')\n",
    "    axes[0, 1].set_title('Kernel Density Estimate (KDE)', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Laboratory Value')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(lab_values, dist=\"norm\", plot=axes[1, 0])\n",
    "    axes[1, 0].set_title('Q-Q Plot: Normality Assessment', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Summary statistics text box\n",
    "    stats_text = f'Mean: {lab_values.mean():.2f}\\nMedian: {lab_values.median():.2f}\\nStd Dev: {lab_values.std():.2f}\\n'\n",
    "    stats_text += f'Min: {lab_values.min():.2f}\\nMax: {lab_values.max():.2f}\\nSkewness: {skew(lab_values):.2f}'\n",
    "    axes[1, 1].text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), family='monospace')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\n**Interpretation:**')\n",
    "    print(f'‚Ä¢ The histogram shows the frequency distribution of laboratory values.')\n",
    "    print(f'‚Ä¢ The KDE plot provides a smooth estimate of the probability density.')\n",
    "    print(f'‚Ä¢ The Q-Q plot compares against normal distribution (points on diagonal = normal).')\n",
    "    print(f'‚Ä¢ Mean: {lab_values.mean():.2f}, Median: {lab_values.median():.2f}, Std Dev: {lab_values.std():.2f}')\n",
    "    print(f'‚Ä¢ Skewness: {skew(lab_values):.2f} (positive = right-skewed, negative = left-skewed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient demographics analysis\n",
    "if df_patients is not None and 'gender' in df_patients.columns:\n",
    "    print('\\n[2] Distribution of Patient Demographics')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Gender distribution - Bar plot\n",
    "    gender_counts = df_patients['gender'].value_counts()\n",
    "    colors = ['#1f77b4', '#ff7f0e']\n",
    "    bars = axes[0].bar(gender_counts.index, gender_counts.values, color=colors, edgecolor='black', alpha=0.8)\n",
    "    axes[0].set_title('Bar Plot: Patient Gender Distribution', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_xlabel('Gender')\n",
    "    axes[0].set_ylabel('Number of Patients')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Gender distribution - Pie chart\n",
    "    colors_pie = ['#1f77b4', '#ff7f0e']\n",
    "    wedges, texts, autotexts = axes[1].pie(gender_counts.values, labels=gender_counts.index, \n",
    "                                            autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "    axes[1].set_title('Pie Chart: Gender Proportion', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\n**Interpretation:**')\n",
    "    print(f'‚Ä¢ Total patients: {len(df_patients):,}')\n",
    "    for gender, count in gender_counts.items():\n",
    "        pct = count / len(df_patients) * 100\n",
    "        print(f'‚Ä¢ {gender}: {count:,} ({pct:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0c4c4",
   "metadata": {},
   "source": [
    "## Section 7: Correlation Analysis Between Clinical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc306e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print('\\nCorrelation Analysis: Clinical Variables')\n",
    "print('='*80)\n",
    "\n",
    "if df_labevents is not None and 'subject_id' in df_labevents.columns:\n",
    "    # Create pivot table\n",
    "    df_lab_clean = df_labevents[['subject_id', 'itemid', 'valuenum']].dropna()\n",
    "    pivot_data = df_lab_clean.pivot_table(\n",
    "        index='subject_id',\n",
    "        columns='itemid',\n",
    "        values='valuenum',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Select top features\n",
    "    top_items = pivot_data.notna().sum().nlargest(12).index\n",
    "    pivot_subset = pivot_data[top_items].dropna(how='all').fillna(pivot_data[top_items].mean())\n",
    "    \n",
    "    if pivot_subset.shape[0] > 2:\n",
    "        # Correlation matrix\n",
    "        corr_matrix = pivot_subset.corr()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(13, 11))\n",
    "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                   square=True, ax=ax, cbar_kws={'label': 'Pearson Correlation'}, \n",
    "                   vmin=-1, vmax=1, linewidths=0.5)\n",
    "        ax.set_title('Correlation Matrix: Top 12 Laboratory Items', fontsize=12, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f'\\n**Interpretation:**')\n",
    "        print(f'‚Ä¢ This heatmap shows pairwise Pearson correlations between the top laboratory items.')\n",
    "        print(f'‚Ä¢ Values close to +1 (dark red) indicate strong positive correlation.')\n",
    "        print(f'‚Ä¢ Values close to -1 (dark blue) indicate strong negative correlation.')\n",
    "        print(f'‚Ä¢ Values close to 0 (white) indicate weak or no linear correlation.')\n",
    "        print(f'\\n‚Ä¢ Top 5 strongest correlations (excluding diagonal):')\n",
    "        \n",
    "        # Find top correlations\n",
    "        corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_pairs.append({\n",
    "                    'Item1': corr_matrix.columns[i],\n",
    "                    'Item2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "        \n",
    "        top_corrs = sorted(corr_pairs, key=lambda x: abs(x['Correlation']), reverse=True)[:5]\n",
    "        for idx, corr in enumerate(top_corrs, 1):\n",
    "            print(f'  {idx}. Items {corr[\"Item1\"]} ‚Üî {corr[\"Item2\"]}: r = {corr[\"Correlation\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83acbd",
   "metadata": {},
   "source": [
    "## Section 8: Class Balancing Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class balancing analysis\n",
    "print('\\nClass Balancing Assessment')\n",
    "print('='*80)\n",
    "\n",
    "if df_admissions is not None and 'hospital_expire_flag' in df_admissions.columns:\n",
    "    # Get unique admissions per patient\n",
    "    df_admit_unique = df_admissions.drop_duplicates(subset=['subject_id'], keep='first')\n",
    "    mortality = df_admit_unique['hospital_expire_flag'].value_counts().sort_index()\n",
    "    mortality_pct = mortality / len(df_admit_unique) * 100\n",
    "    \n",
    "    print(f'\\nTarget Variable: Hospital Mortality (hospital_expire_flag)')\n",
    "    print(f'Total patients: {len(df_admit_unique):,}')\n",
    "    print(f'\\nClass Distribution:')\n",
    "    print(f'  Class 0 (Survived): {mortality[0]:,} ({mortality_pct[0]:.2f}%)')\n",
    "    print(f'  Class 1 (Died): {mortality[1]:,} ({mortality_pct[1]:.2f}%)')\n",
    "    \n",
    "    imbalance_ratio = max(mortality.values) / min(mortality.values)\n",
    "    print(f'\\nImbalance Ratio: {imbalance_ratio:.2f}:1')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar plot\n",
    "    colors_bar = ['#2ecc71', '#e74c3c']  # Green for survived, red for died\n",
    "    bars = axes[0].bar(['Survived (0)', 'Died (1)'], mortality.values, color=colors_bar, edgecolor='black', alpha=0.8)\n",
    "    axes[0].set_title('Bar Plot: Hospital Mortality Distribution', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_ylabel('Number of Patients')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    explode = (0.05, 0.1)\n",
    "    axes[1].pie(mortality.values, labels=['Survived', 'Died'], autopct='%1.1f%%',\n",
    "               colors=colors_bar, explode=explode, startangle=90, shadow=True)\n",
    "    axes[1].set_title('Pie Chart: Mortality Proportion', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\n**Interpretation:**')\n",
    "    if imbalance_ratio > 2:\n",
    "        print(f'‚ö†Ô∏è  SIGNIFICANT CLASS IMBALANCE DETECTED!')\n",
    "        print(f'   Recommendation: Apply balancing techniques')\n",
    "        print(f'   ‚Ä¢ SMOTE (Synthetic Minority Over-sampling): Recommended')\n",
    "        print(f'   ‚Ä¢ Random Under-sampling: Alternative approach')\n",
    "        print(f'   ‚Ä¢ Weighted loss functions: For model training')\n",
    "    else:\n",
    "        print(f'‚úì Classes are relatively balanced. Balancing may not be necessary.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454377e",
   "metadata": {},
   "source": [
    "## Section 9: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ef9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and preprocessing\n",
    "print('\\nData Cleaning and Preprocessing')\n",
    "print('='*80)\n",
    "\n",
    "# Step 1: Clean admissions\n",
    "if df_admissions is not None:\n",
    "    print('\\n[Step 1] Cleaning Admissions Table')\n",
    "    df_admit_clean = df_admissions.drop_duplicates()\n",
    "    df_admit_first = df_admit_clean.drop_duplicates(subset=['subject_id'], keep='first')\n",
    "    print(f'  Original records: {len(df_admissions):,}')\n",
    "    print(f'  After deduplication: {len(df_admit_clean):,}')\n",
    "    print(f'  First admission per patient: {len(df_admit_first):,}')\n",
    "\n",
    "# Step 2: Create feature matrix from labevents\n",
    "if df_labevents is not None:\n",
    "    print('\\n[Step 2] Creating Patient-Laboratory Feature Matrix')\n",
    "    \n",
    "    df_lab_valid = df_labevents[['subject_id', 'itemid', 'valuenum']].dropna(subset=['valuenum'])\n",
    "    print(f'  Valid lab events: {len(df_lab_valid):,}')\n",
    "    \n",
    "    # Aggregate by mean\n",
    "    df_lab_agg = df_lab_valid.groupby(['subject_id', 'itemid'])['valuenum'].mean().reset_index()\n",
    "    \n",
    "    # Pivot\n",
    "    X_matrix = df_lab_agg.pivot(index='subject_id', columns='itemid', values='valuenum')\n",
    "    print(f'  Feature matrix shape: {X_matrix.shape}')\n",
    "    \n",
    "    sparsity_pct = (X_matrix.isnull().sum().sum() / X_matrix.size * 100)\n",
    "    print(f'  Sparsity: {sparsity_pct:.2f}%')\n",
    "    \n",
    "    # Impute with median\n",
    "    X_matrix_imputed = X_matrix.fillna(X_matrix.median())\n",
    "    print(f'  After imputation: {X_matrix_imputed.isnull().sum().sum()} missing values')\n",
    "    \n",
    "    # Select top features by variance\n",
    "    top_features = X_matrix_imputed.var().nlargest(25).index\n",
    "    X_final = X_matrix_imputed[top_features]\n",
    "    print(f'  Selected top {len(top_features)} features by variance')\n",
    "\n",
    "# Step 3: Prepare target variable\n",
    "if df_admit_first is not None and 'hospital_expire_flag' in df_admit_first.columns:\n",
    "    print('\\n[Step 3] Preparing Target Variable')\n",
    "    \n",
    "    y_target = df_admit_first.set_index('subject_id')[['hospital_expire_flag']]\n",
    "    \n",
    "    # Align X and y\n",
    "    common_patients = X_final.index.intersection(y_target.index)\n",
    "    X_aligned = X_final.loc[common_patients]\n",
    "    y_aligned = y_target.loc[common_patients, 'hospital_expire_flag'].astype(int)\n",
    "    \n",
    "    print(f'  Common patients: {len(common_patients):,}')\n",
    "    print(f'  Final X shape: {X_aligned.shape}')\n",
    "    print(f'  Final y shape: {y_aligned.shape}')\n",
    "    print(f'  Target distribution:')\n",
    "    print(f'    Class 0 (Survived): {(y_aligned == 0).sum():,}')\n",
    "    print(f'    Class 1 (Died): {(y_aligned == 1).sum():,}')\n",
    "else:\n",
    "    X_aligned = X_final\n",
    "    y_aligned = None\n",
    "    print('\\n‚ö†Ô∏è  Target variable not available. Using features only.')\n",
    "\n",
    "print('\\n‚úì Data preprocessing completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da0778",
   "metadata": {},
   "source": [
    "## Section 10: Train-Test Split and Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe935918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split and export\n",
    "print('\\nTrain-Test Split and Data Export')\n",
    "print('='*80)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '/Users/michi/Desktop/IntercicloEstocasticos/ExamenPractico/processed_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f'\\n[1] Output directory: {output_dir}')\n",
    "\n",
    "# Perform train-test split\n",
    "print('\\n[2] Stratified Train-Test Split (80-20)')\n",
    "\n",
    "if y_aligned is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aligned, y_aligned,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_aligned\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test = train_test_split(\n",
    "        X_aligned,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    y_train = None\n",
    "    y_test = None\n",
    "\n",
    "print(f'  Training set: {X_train.shape[0]:,} samples √ó {X_train.shape[1]} features')\n",
    "print(f'  Test set: {X_test.shape[0]:,} samples √ó {X_test.shape[1]} features')\n",
    "\n",
    "if y_train is not None:\n",
    "    print(f'\\n  Train class distribution:')\n",
    "    print(f'    Survived: {(y_train == 0).sum():,} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)')\n",
    "    print(f'    Died: {(y_train == 1).sum():,} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)')\n",
    "    print(f'\\n  Test class distribution:')\n",
    "    print(f'    Survived: {(y_test == 0).sum():,} ({(y_test == 0).sum()/len(y_test)*100:.1f}%)')\n",
    "    print(f'    Died: {(y_test == 1).sum():,} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)')\n",
    "\n",
    "# Standardize features\n",
    "print('\\n[3] Feature Standardization (StandardScaler)')\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "print(f'  Scaling completed (mean=0, std=1)')\n",
    "\n",
    "# Export datasets\n",
    "print('\\n[4] Exporting Datasets')\n",
    "\n",
    "# Training set\n",
    "train_df = X_train_scaled_df.copy()\n",
    "if y_train is not None:\n",
    "    train_df['hospital_expire_flag'] = y_train\n",
    "\n",
    "train_path = os.path.join(output_dir, 'data_train.csv')\n",
    "train_df.to_csv(train_path)\n",
    "print(f'  ‚úì Training set: {train_path}')\n",
    "\n",
    "# Test set\n",
    "test_df = X_test_scaled_df.copy()\n",
    "if y_test is not None:\n",
    "    test_df['hospital_expire_flag'] = y_test\n",
    "\n",
    "test_path = os.path.join(output_dir, 'data_test.csv')\n",
    "test_df.to_csv(test_path)\n",
    "print(f'  ‚úì Test set: {test_path}')\n",
    "\n",
    "# Complete dataset\n",
    "complete_df = pd.concat([train_df, test_df])\n",
    "complete_df['data_split'] = ['train'] * len(train_df) + ['test'] * len(test_df)\n",
    "\n",
    "complete_path = os.path.join(output_dir, 'data_prepared.csv')\n",
    "complete_df.to_csv(complete_path)\n",
    "print(f'  ‚úì Complete dataset: {complete_path}')\n",
    "\n",
    "# Pickle format\n",
    "pickle_path = os.path.join(output_dir, 'data_prepared.pkl')\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train_scaled_df,\n",
    "        'X_test': X_test_scaled_df,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': X_train.columns.tolist()\n",
    "    }, f)\n",
    "print(f'  ‚úì Pickle file: {pickle_path}')\n",
    "\n",
    "# Metadata\n",
    "metadata = {\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features': X_train.shape[1],\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'target_variable': 'hospital_expire_flag',\n",
    "    'train_test_ratio': '80-20',\n",
    "    'scaling_method': 'StandardScaler',\n",
    "    'export_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(output_dir, 'data_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4, default=str)\n",
    "print(f'  ‚úì Metadata: {metadata_path}')\n",
    "\n",
    "print('\\n‚úì All datasets exported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc200796",
   "metadata": {},
   "source": [
    "## Section 11: Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print('\\n' + '='*80)\n",
    "print('EXPLORATORY DATA ANALYSIS - FINAL SUMMARY REPORT')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nüìä DATA SOURCES:')\n",
    "print(f'  ‚Ä¢ MIMIC-III Clinical Database (Kaggle Hub)')\n",
    "print(f'  ‚Ä¢ Tables: Admissions, Patients, Lab Events, Lab Items')\n",
    "\n",
    "print(f'\\nüìà DATASET OVERVIEW:')\n",
    "print(f'  ‚Ä¢ Original admissions: {len(df_admissions) if df_admissions is not None else \"N/A\":,}')\n",
    "print(f'  ‚Ä¢ Unique patients: {len(df_patients) if df_patients is not None else \"N/A\":,}')\n",
    "print(f'  ‚Ä¢ Laboratory events: {len(df_labevents) if df_labevents is not None else \"N/A\":,}')\n",
    "\n",
    "print(f'\\nüîç DATA QUALITY ASSESSMENT:')\n",
    "print(f'  ‚úì Missing values: Detected and documented')\n",
    "print(f'  ‚úì Duplicates: Removed {len(df_admissions) - len(df_admit_clean) if df_admissions is not None else 0}')\n",
    "print(f'  ‚úì Outliers: Detected using IQR method')\n",
    "print(f'  ‚úì Sparsity: Characterized and imputed')\n",
    "\n",
    "print(f'\\nüìä FINAL DATASET:')\n",
    "print(f'  ‚Ä¢ Training samples: {len(X_train):,}')\n",
    "print(f'  ‚Ä¢ Test samples: {len(X_test):,}')\n",
    "print(f'  ‚Ä¢ Total features: {X_train.shape[1]}')\n",
    "print(f'  ‚Ä¢ Train-Test split: 80-20')\n",
    "print(f'  ‚Ä¢ Feature scaling: StandardScaler (mean=0, std=1)')\n",
    "\n",
    "print(f'\\nüíæ EXPORTED FILES:')\n",
    "print(f'  1. data_train.csv - Training dataset')\n",
    "print(f'  2. data_test.csv - Test dataset')\n",
    "print(f'  3. data_prepared.csv - Complete dataset with split indicator')\n",
    "print(f'  4. data_prepared.pkl - Python pickle format')\n",
    "print(f'  5. data_metadata.json - Metadata and data dictionary')\n",
    "print(f'\\n  üìÇ Location: {output_dir}')\n",
    "\n",
    "print(f'\\n‚úÖ EDA PROCESS COMPLETED!')\n",
    "print(f'‚è∞ Timestamp: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
